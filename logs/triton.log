
=============================
== Triton Inference Server ==
=============================

NVIDIA Release 23.07 (build 64724866)
Triton Server Version 2.36.0

Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

I1004 03:11:46.406132 1 libtorch.cc:2507] TRITONBACKEND_Initialize: pytorch
I1004 03:11:46.406179 1 libtorch.cc:2517] Triton TRITONBACKEND API version: 1.13
I1004 03:11:46.406183 1 libtorch.cc:2523] 'pytorch' TRITONBACKEND API version: 1.13
I1004 03:11:47.557338 1 pinned_memory_manager.cc:241] Pinned memory pool is created at '0x205000000' with size 268435456
I1004 03:11:47.558624 1 cuda_memory_manager.cc:107] CUDA memory pool is created on device 0 with size 67108864
I1004 03:11:47.562888 1 model_lifecycle.cc:462] loading: classnet:2
I1004 03:11:47.565023 1 tensorrt.cc:65] TRITONBACKEND_Initialize: tensorrt
I1004 03:11:47.565054 1 tensorrt.cc:75] Triton TRITONBACKEND API version: 1.13
I1004 03:11:47.565057 1 tensorrt.cc:81] 'tensorrt' TRITONBACKEND API version: 1.13
I1004 03:11:47.565059 1 tensorrt.cc:105] backend configuration:
{"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}}
I1004 03:11:47.599973 1 tensorrt.cc:222] TRITONBACKEND_ModelInitialize: classnet (version 2)
I1004 03:11:48.502115 1 logging.cc:46] Loaded engine size: 224 MiB
I1004 03:11:48.628981 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +219, now: CPU 0, GPU 219 (MiB)
W1004 03:11:48.633155 1 model_state.cc:530] The specified dimensions in model config for classnet hints that batching is unavailable
I1004 03:11:48.645190 1 tensorrt.cc:288] TRITONBACKEND_ModelInstanceInitialize: classnet_0 (GPU device 0)
I1004 03:11:48.772957 1 logging.cc:46] Loaded engine size: 224 MiB
I1004 03:11:48.853514 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +219, now: CPU 0, GPU 219 (MiB)
I1004 03:11:48.865567 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +94, now: CPU 0, GPU 313 (MiB)
W1004 03:11:48.865596 1 logging.cc:43] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See "Lazy Loading" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading
I1004 03:11:48.869421 1 instance_state.cc:188] Created instance classnet_0 on GPU 0 with stream priority 0 and optimization profile default[0];
I1004 03:11:48.870415 1 model_lifecycle.cc:817] successfully loaded 'classnet'
I1004 03:11:48.870562 1 server.cc:604] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I1004 03:11:48.870614 1 server.cc:631] 
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend  | Path                                                      | Config                                                                                                                                                        |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| pytorch  | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so   | {}                                                                                                                                                            |
| tensorrt | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so | {"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}} |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I1004 03:11:48.870908 1 server.cc:674] 
+----------+---------+--------+
| Model    | Version | Status |
+----------+---------+--------+
| classnet | 2       | READY  |
+----------+---------+--------+

I1004 03:11:48.921248 1 metrics.cc:810] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3080 Ti Laptop GPU
I1004 03:11:48.921458 1 metrics.cc:703] Collecting CPU metrics
I1004 03:11:48.921836 1 tritonserver.cc:2415] 
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                                          |
| server_version                   | 2.36.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | /models                                                                                                                                                                                                         |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 0                                                                                                                                                                                                               |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I1004 03:11:48.936993 1 grpc_server.cc:2451] Started GRPCInferenceService at 0.0.0.0:8001
I1004 03:11:48.938015 1 http_server.cc:3558] Started HTTPService at 0.0.0.0:8000
I1004 03:11:48.979840 1 http_server.cc:187] Started Metrics Service at 0.0.0.0:8002
W1004 03:11:49.927443 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
W1004 03:11:50.927866 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
W1004 03:11:51.930170 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
