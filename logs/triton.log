
=============================
== Triton Inference Server ==
=============================

NVIDIA Release 23.07 (build 64724866)
Triton Server Version 2.36.0

Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

I0224 07:28:13.128742 1 libtorch.cc:2507] TRITONBACKEND_Initialize: pytorch
I0224 07:28:13.128789 1 libtorch.cc:2517] Triton TRITONBACKEND API version: 1.13
I0224 07:28:13.128804 1 libtorch.cc:2523] 'pytorch' TRITONBACKEND API version: 1.13
I0224 07:28:13.432020 1 pinned_memory_manager.cc:241] Pinned memory pool is created at '0x205000000' with size 268435456
I0224 07:28:13.433342 1 cuda_memory_manager.cc:107] CUDA memory pool is created on device 0 with size 67108864
I0224 07:28:13.440396 1 model_lifecycle.cc:462] loading: classnet:2
I0224 07:28:13.440659 1 model_lifecycle.cc:462] loading: demo:1
I0224 07:28:13.442810 1 tensorrt.cc:65] TRITONBACKEND_Initialize: tensorrt
I0224 07:28:13.442829 1 tensorrt.cc:75] Triton TRITONBACKEND API version: 1.13
I0224 07:28:13.442831 1 tensorrt.cc:81] 'tensorrt' TRITONBACKEND API version: 1.13
I0224 07:28:13.442833 1 tensorrt.cc:105] backend configuration:
{"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}}
I0224 07:28:13.443341 1 tensorrt.cc:222] TRITONBACKEND_ModelInitialize: classnet (version 2)
I0224 07:28:13.443347 1 tensorrt.cc:222] TRITONBACKEND_ModelInitialize: demo (version 1)
I0224 07:28:13.445709 1 logging.cc:46] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
I0224 07:28:13.704925 1 logging.cc:46] Loaded engine size: 224 MiB
I0224 07:28:13.726491 1 logging.cc:46] Loaded engine size: 239 MiB
W0224 07:28:13.741683 1 logging.cc:43] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
W0224 07:28:13.741683 1 logging.cc:43] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
I0224 07:28:13.874913 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +453, now: CPU 0, GPU 453 (MiB)
I0224 07:28:13.874915 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +453, now: CPU 0, GPU 453 (MiB)
W0224 07:28:13.875694 1 model_state.cc:530] The specified dimensions in model config for classnet hints that batching is unavailable
W0224 07:28:13.876567 1 model_state.cc:530] The specified dimensions in model config for demo hints that batching is unavailable
I0224 07:28:13.891651 1 tensorrt.cc:288] TRITONBACKEND_ModelInstanceInitialize: classnet_0 (GPU device 0)
I0224 07:28:13.891884 1 tensorrt.cc:288] TRITONBACKEND_ModelInstanceInitialize: demo_0 (GPU device 0)
I0224 07:28:13.972874 1 logging.cc:46] Loaded engine size: 224 MiB
W0224 07:28:13.972940 1 logging.cc:43] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
I0224 07:28:14.032560 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +219, now: CPU 0, GPU 219 (MiB)
I0224 07:28:14.033320 1 logging.cc:46] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
I0224 07:28:14.041555 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +94, now: CPU 0, GPU 313 (MiB)
I0224 07:28:14.043253 1 instance_state.cc:188] Created instance classnet_0 on GPU 0 with stream priority 0 and optimization profile default[0];
I0224 07:28:14.043717 1 model_lifecycle.cc:817] successfully loaded 'classnet'
I0224 07:28:14.120602 1 logging.cc:46] Loaded engine size: 239 MiB
W0224 07:28:14.120680 1 logging.cc:43] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
I0224 07:28:14.181207 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +234, now: CPU 0, GPU 547 (MiB)
I0224 07:28:14.186828 1 logging.cc:46] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +100, now: CPU 1, GPU 647 (MiB)
I0224 07:28:14.187583 1 instance_state.cc:188] Created instance demo_0 on GPU 0 with stream priority 0 and optimization profile default[0];
I0224 07:28:14.187889 1 model_lifecycle.cc:817] successfully loaded 'demo'
I0224 07:28:14.187970 1 server.cc:604] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0224 07:28:14.188029 1 server.cc:631] 
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend  | Path                                                      | Config                                                                                                                                                        |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| pytorch  | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so   | {}                                                                                                                                                            |
| tensorrt | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so | {"cmdline":{"auto-complete-config":"true","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size":"4"}} |
+----------+-----------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0224 07:28:14.188060 1 server.cc:674] 
+----------+---------+--------+
| Model    | Version | Status |
+----------+---------+--------+
| classnet | 2       | READY  |
| demo     | 1       | READY  |
+----------+---------+--------+

I0224 07:28:14.222429 1 metrics.cc:810] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3080 Ti Laptop GPU
I0224 07:28:14.222546 1 metrics.cc:703] Collecting CPU metrics
I0224 07:28:14.222672 1 tritonserver.cc:2415] 
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                                          |
| server_version                   | 2.36.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | /models                                                                                                                                                                                                         |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 0                                                                                                                                                                                                               |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0224 07:28:14.240217 1 grpc_server.cc:2451] Started GRPCInferenceService at 0.0.0.0:8001
I0224 07:28:14.240373 1 http_server.cc:3558] Started HTTPService at 0.0.0.0:8000
I0224 07:28:14.282735 1 http_server.cc:187] Started Metrics Service at 0.0.0.0:8002
W0224 07:28:15.224953 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
W0224 07:28:16.225432 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
W0224 07:28:17.227845 1 metrics.cc:575] Unable to get power limit for GPU 0. Status:Success, value:0.000000
Signal (15) received.
I0224 07:34:33.964038 1 server.cc:305] Waiting for in-flight requests to complete.
I0224 07:34:33.964061 1 server.cc:321] Timeout 30: Found 0 model versions that have in-flight inferences
I0224 07:34:33.964309 1 tensorrt.cc:344] TRITONBACKEND_ModelInstanceFinalize: delete instance state
I0224 07:34:33.964328 1 server.cc:336] All models are stopped, unloading models
I0224 07:34:33.964344 1 server.cc:343] Timeout 30: Found 2 live models and 0 in-flight non-inference requests
I0224 07:34:33.964535 1 tensorrt.cc:344] TRITONBACKEND_ModelInstanceFinalize: delete instance state
I0224 07:34:33.966871 1 tensorrt.cc:265] TRITONBACKEND_ModelFinalize: delete model state
I0224 07:34:33.966889 1 tensorrt.cc:265] TRITONBACKEND_ModelFinalize: delete model state
I0224 07:34:33.981333 1 model_lifecycle.cc:608] successfully unloaded 'classnet' version 2
I0224 07:34:33.983546 1 model_lifecycle.cc:608] successfully unloaded 'demo' version 1
I0224 07:34:34.964438 1 server.cc:343] Timeout 29: Found 0 live models and 0 in-flight non-inference requests
